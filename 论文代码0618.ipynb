{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346ad7c-1295-4f8b-b9bb-9cd28e4c3a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7a2cba-c9d6-481a-9583-544a46b5a96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "时间序列交叉验证（3 fold）- 30因子模型稳健性验证\n",
      "数据分4份：训练集递增，测试集为下一个1/4\n",
      "============================================================\n",
      "总时间点数: 482\n",
      "每个1/4包含时间点数: 120\n",
      "时间范围: 1/10/22 到 9/9/22\n",
      "\n",
      "--- 第 1 折验证 ---\n",
      "训练时间范围: 1/10/22 到 12/15/21 (共120个时间点)\n",
      "测试时间范围: 12/15/22 到 4/10/23 (共120个时间点)\n",
      "训练集占比: 24.9%, 测试集占比: 24.9%\n",
      "训练样本数: 224183, 测试样本数: 225217\n",
      "早停于第143轮，最佳损失: 0.423661\n",
      "第1折结果:\n",
      "  R²: 0.1722, 调整R²: 0.1721\n",
      "  MSE: 0.780472, MAE: 0.593374\n",
      "  信息比率: 0.0001, 跟踪误差: 14.0242\n",
      "  训练轮数: 143\n",
      "\n",
      "--- 第 2 折验证 ---\n",
      "训练时间范围: 1/10/22 到 4/10/23 (共240个时间点)\n",
      "测试时间范围: 4/11/22 到 7/11/23 (共120个时间点)\n",
      "训练集占比: 49.8%, 测试集占比: 24.9%\n",
      "训练样本数: 449400, 测试样本数: 224261\n",
      "早停于第171轮，最佳损失: 0.427456\n",
      "第2折结果:\n",
      "  R²: 0.1933, 调整R²: 0.1932\n",
      "  MSE: 0.932847, MAE: 0.663227\n",
      "  信息比率: -0.0070, 跟踪误差: 15.3322\n",
      "  训练轮数: 171\n",
      "\n",
      "--- 第 3 折验证 ---\n",
      "训练时间范围: 1/10/22 到 7/11/23 (共360个时间点)\n",
      "测试时间范围: 7/12/22 到 9/9/22 (共122个时间点)\n",
      "训练集占比: 74.7%, 测试集占比: 25.3%\n",
      "训练样本数: 673661, 测试样本数: 228106\n",
      "早停于第127轮，最佳损失: 0.516831\n",
      "第3折结果:\n",
      "  R²: 0.1814, 调整R²: 0.1813\n",
      "  MSE: 0.739761, MAE: 0.581417\n",
      "  信息比率: -0.0001, 跟踪误差: 13.6536\n",
      "  训练轮数: 127\n",
      "\n",
      "============================================================\n",
      "=== 交叉验证结果汇总 ===\n",
      "============================================================\n",
      "R² - 均值: 0.1823 ± 0.0105\n",
      "调整R² - 均值: 0.1822 ± 0.0106\n",
      "MSE - 均值: 0.817693 ± 0.101782\n",
      "信息比率 - 均值: -0.0024 ± 0.0041\n",
      "\n",
      "=== 稳健性评估 ===\n",
      "R²变异系数: 0.0579\n",
      "R²稳健性评估: 稳健\n",
      "\n",
      "=== 性能趋势分析 ===\n",
      "第1折 (训练集占24.9%): R² = 0.1722\n",
      "第2折 (训练集占49.8%): R² = 0.1933\n",
      "第3折 (训练集占74.7%): R² = 0.1814\n",
      "\n",
      "=== 保存的CSV文件 ===\n",
      "1. 交叉验证主要结果: cross_validation_results.csv\n",
      "2. 统计汇总: cross_validation_summary.csv\n",
      "3. 因子系数汇总: cross_validation_coefficients.csv\n",
      "4. 各折预测详情: fold_1_predictions.csv, fold_2_predictions.csv, fold_3_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# 正确的时间序列交叉验证（3 fold，数据分4份）\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"时间序列交叉验证（3 fold）- 30因子模型稳健性验证\")\n",
    "print(\"数据分4份：训练集递增，测试集为下一个1/4\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "data_cv = pd.read_csv('/Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data16_processed.csv')\n",
    "data_cv[numeric_cols] = data_cv[numeric_cols].fillna(data_cv[numeric_cols].mean())\n",
    "\n",
    "for i, j in combinations(range(len(original_features)), 2):\n",
    "  col_name = f'interaction_{i}_{j}'\n",
    "  data_cv[col_name] = data_cv[original_features[i]] * data_cv[original_features[j]]\n",
    "\n",
    "X_cv = data_cv[all_features].values\n",
    "y_cv = data_cv['Return'].values\n",
    "time_cv = data_cv.iloc[:, 1].values\n",
    "\n",
    "unique_times_cv = np.unique(time_cv)\n",
    "n_times = len(unique_times_cv)\n",
    "quarter_size = n_times // 4  # 每个1/4的大小\n",
    "\n",
    "print(f\"总时间点数: {n_times}\")\n",
    "print(f\"每个1/4包含时间点数: {quarter_size}\")\n",
    "print(f\"时间范围: {unique_times_cv[0]} 到 {unique_times_cv[-1]}\")\n",
    "\n",
    "cv_results = []\n",
    "cv_detailed_results = []\n",
    "\n",
    "for fold in range(3):\n",
    "  print(f\"\\n--- 第 {fold+1} 折验证 ---\")\n",
    "\n",
    "  train_end_idx = quarter_size * (fold + 1)  # 训练集结束位置\n",
    "  test_start_idx = train_end_idx             # 测试集开始位置\n",
    "  test_end_idx = quarter_size * (fold + 2)   # 测试集结束位置\n",
    "  \n",
    "  # 最后一折的测试集包含所有剩余数据\n",
    "  if fold == 2:\n",
    "      test_end_idx = n_times\n",
    "  \n",
    "  train_times = unique_times_cv[:train_end_idx]\n",
    "  test_times = unique_times_cv[test_start_idx:test_end_idx]\n",
    "  \n",
    "  print(f\"训练时间范围: {train_times[0]} 到 {train_times[-1]} (共{len(train_times)}个时间点)\")\n",
    "  print(f\"测试时间范围: {test_times[0]} 到 {test_times[-1]} (共{len(test_times)}个时间点)\")\n",
    "  print(f\"训练集占比: {len(train_times)/n_times:.1%}, 测试集占比: {len(test_times)/n_times:.1%}\")\n",
    "  \n",
    "\n",
    "  assert len(np.intersect1d(train_times, test_times)) == 0, f\"第{fold+1}折: 训练集和测试集时间重叠!\"\n",
    "  assert len(train_times) > 0 and len(test_times) > 0, f\"第{fold+1}折: 训练集或测试集为空!\"\n",
    "  assert train_times[-1] < test_times[0], f\"第{fold+1}折: 训练集时间应该早于测试集时间!\"\n",
    "  \n",
    "  train_mask = np.isin(time_cv, train_times)\n",
    "  test_mask = np.isin(time_cv, test_times)\n",
    "  \n",
    "  X_train_cv, X_test_cv = X_cv[train_mask], X_cv[test_mask]\n",
    "  y_train_cv, y_test_cv = y_cv[train_mask], y_cv[test_mask]\n",
    "  time_train_cv, time_test_cv = time_cv[train_mask], time_cv[test_mask]\n",
    "  \n",
    "  print(f\"训练样本数: {np.sum(train_mask)}, 测试样本数: {np.sum(test_mask)}\")\n",
    "  \n",
    "  if np.sum(train_mask) < 100:\n",
    "      print(f\"警告: 第{fold+1}折训练样本数过少 ({np.sum(train_mask)})\")\n",
    "  if np.sum(test_mask) < 50:\n",
    "      print(f\"警告: 第{fold+1}折测试样本数过少 ({np.sum(test_mask)})\")\n",
    "  \n",
    "  scaler_cv = StandardScaler()\n",
    "  X_train_scaled_cv = scaler_cv.fit_transform(X_train_cv)\n",
    "  X_test_scaled_cv = scaler_cv.transform(X_test_cv)\n",
    "  \n",
    "  model_cv = DualStructureNN(num_factors=30, num_features=X_train_scaled_cv.shape[1])\n",
    "  optimizer_cv = keras.optimizers.Adam(learning_rate=0.001)\n",
    "  \n",
    "  train_time_data_cv, unique_times_train_cv = prepare_time_based_data(X_train_scaled_cv, y_train_cv, time_train_cv)\n",
    "  \n",
    "  training_losses = []\n",
    "  best_loss = float('inf')\n",
    "  patience_counter = 0\n",
    "  patience = 10\n",
    "  \n",
    "  for epoch in range(200):\n",
    "      epoch_losses = []\n",
    "      for t in unique_times_train_cv:\n",
    "          t_data = train_time_data_cv[t]\n",
    "          if len(t_data['X']) < 2: continue\n",
    "          x_weighted = np.dot(t_data['X'].T, t_data['y']) / len(t_data['y'])\n",
    "          with tf.GradientTape() as tape:\n",
    "              predictions, _, _ = model_cv([tf.constant(t_data['X'], dtype=tf.float32), tf.constant(x_weighted, dtype=tf.float32)], training=True)\n",
    "              loss = tf.reduce_mean(tf.square(tf.constant(t_data['y'], dtype=tf.float32) - predictions))\n",
    "          gradients = tape.gradient(loss, model_cv.trainable_variables)\n",
    "          optimizer_cv.apply_gradients(zip(gradients, model_cv.trainable_variables))\n",
    "          epoch_losses.append(loss.numpy())\n",
    "      \n",
    "      if epoch_losses:\n",
    "          current_loss = np.mean(epoch_losses)\n",
    "          training_losses.append(current_loss)\n",
    "          \n",
    "          # 早停机制\n",
    "          if current_loss < best_loss:\n",
    "              best_loss = current_loss\n",
    "              patience_counter = 0\n",
    "          else:\n",
    "              patience_counter += 1\n",
    "              \n",
    "          if patience_counter >= patience:\n",
    "              print(f\"早停于第{epoch+1}轮，最佳损失: {best_loss:.6f}\")\n",
    "              break\n",
    "  \n",
    "  test_time_data_cv, unique_times_test_cv = prepare_time_based_data(X_test_scaled_cv, y_test_cv, time_test_cv)\n",
    "  all_factors_cv = []\n",
    "  test_indices_cv = []\n",
    "  \n",
    "  for t in unique_times_test_cv:\n",
    "      t_data = test_time_data_cv[t]\n",
    "      if len(t_data['X']) < 2: continue\n",
    "      x_weighted = np.dot(t_data['X'].T, t_data['y']) / len(t_data['y'])\n",
    "      _, _, factors = model_cv([tf.constant(t_data['X'], dtype=tf.float32), tf.constant(x_weighted, dtype=tf.float32)], training=False)\n",
    "      factors_np = factors.numpy()\n",
    "      if len(factors_np.shape) == 1:\n",
    "          factors_expanded = np.tile(factors_np, (len(t_data['X']), 1))\n",
    "      else:\n",
    "          factors_expanded = factors_np\n",
    "      all_factors_cv.extend(factors_expanded)\n",
    "      test_indices_cv.extend(t_data['indices'])\n",
    "  \n",
    "  all_factors_cv = np.array(all_factors_cv)\n",
    "  y_test_subset_cv = y_test_cv[test_indices_cv]\n",
    "  \n",
    "  # 线性回归计算R²\n",
    "  linear_reg_cv = LinearRegression()\n",
    "  linear_reg_cv.fit(all_factors_cv, y_test_subset_cv)\n",
    "  y_pred_cv = linear_reg_cv.predict(all_factors_cv)\n",
    "  r2_cv = r2_score(y_test_subset_cv, y_pred_cv)\n",
    "  \n",
    "  # 计算调整R²和其他指标\n",
    "  n_cv = len(y_test_subset_cv)\n",
    "  adj_r2_cv = 1 - (1 - r2_cv) * (n_cv - 1) / (n_cv - 31)\n",
    "  mse_cv = np.mean((y_test_subset_cv - y_pred_cv)**2)\n",
    "  mae_cv = np.mean(np.abs(y_test_subset_cv - y_pred_cv))\n",
    "  final_loss = training_losses[-1] if training_losses else np.nan\n",
    "  \n",
    "\n",
    "  residuals = y_test_subset_cv - y_pred_cv\n",
    "  tracking_error = np.std(residuals) * np.sqrt(252)  # 年化跟踪误差\n",
    "  info_ratio = (np.mean(residuals) * 252) / tracking_error if tracking_error > 0 else 0\n",
    "  fold_result = {\n",
    "      'Fold': fold + 1,\n",
    "      'Train_Time_Start': str(train_times[0]),\n",
    "      'Train_Time_End': str(train_times[-1]),\n",
    "      'Test_Time_Start': str(test_times[0]),\n",
    "      'Test_Time_End': str(test_times[-1]),\n",
    "      'Train_Samples': int(np.sum(train_mask)),\n",
    "      'Test_Samples': int(n_cv),\n",
    "      'Train_Time_Periods': len(train_times),\n",
    "      'Test_Time_Periods': len(test_times),\n",
    "      'Train_Ratio': len(train_times)/n_times,\n",
    "      'Test_Ratio': len(test_times)/n_times,\n",
    "      'R_Squared': r2_cv,\n",
    "      'Adjusted_R_Squared': adj_r2_cv,\n",
    "      'MSE': mse_cv,\n",
    "      'MAE': mae_cv,\n",
    "      'Information_Ratio': info_ratio,\n",
    "      'Tracking_Error': tracking_error,\n",
    "      'Final_Training_Loss': final_loss,\n",
    "      'Training_Epochs': len(training_losses),\n",
    "      'Factors_Generated': all_factors_cv.shape[1]\n",
    "  }\n",
    "  \n",
    "  cv_results.append(fold_result)\n",
    "  cv_detailed_results.append({\n",
    "      'fold': fold+1, \n",
    "      'factors': all_factors_cv, \n",
    "      'predictions': y_pred_cv, \n",
    "      'actual': y_test_subset_cv,\n",
    "      'coefficients': linear_reg_cv.coef_,\n",
    "      'residuals': residuals\n",
    "  })\n",
    "  \n",
    "  print(f\"第{fold+1}折结果:\")\n",
    "  print(f\"  R²: {r2_cv:.4f}, 调整R²: {adj_r2_cv:.4f}\")\n",
    "  print(f\"  MSE: {mse_cv:.6f}, MAE: {mae_cv:.6f}\")\n",
    "  print(f\"  信息比率: {info_ratio:.4f}, 跟踪误差: {tracking_error:.4f}\")\n",
    "  print(f\"  训练轮数: {len(training_losses)}\")\n",
    "\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "summary_stats = {\n",
    "  'Metric': ['R_Squared', 'Adjusted_R_Squared', 'MSE', 'MAE', 'Information_Ratio'],\n",
    "  'Mean': [\n",
    "      cv_results_df['R_Squared'].mean(),\n",
    "      cv_results_df['Adjusted_R_Squared'].mean(),\n",
    "      cv_results_df['MSE'].mean(),\n",
    "      cv_results_df['MAE'].mean(),\n",
    "      cv_results_df['Information_Ratio'].mean()\n",
    "  ],\n",
    "  'Std': [\n",
    "      cv_results_df['R_Squared'].std(),\n",
    "      cv_results_df['Adjusted_R_Squared'].std(),\n",
    "      cv_results_df['MSE'].std(),\n",
    "      cv_results_df['MAE'].std(),\n",
    "      cv_results_df['Information_Ratio'].std()\n",
    "  ],\n",
    "  'Min': [\n",
    "      cv_results_df['R_Squared'].min(),\n",
    "      cv_results_df['Adjusted_R_Squared'].min(),\n",
    "      cv_results_df['MSE'].min(),\n",
    "      cv_results_df['MAE'].min(),\n",
    "      cv_results_df['Information_Ratio'].min()\n",
    "  ],\n",
    "  'Max': [\n",
    "      cv_results_df['R_Squared'].max(),\n",
    "      cv_results_df['Adjusted_R_Squared'].max(),\n",
    "      cv_results_df['MSE'].max(),\n",
    "      cv_results_df['MAE'].max(),\n",
    "      cv_results_df['Information_Ratio'].max()\n",
    "  ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "\n",
    "coefficients_summary = []\n",
    "for i, result in enumerate(cv_detailed_results):\n",
    "  for j, coef in enumerate(result['coefficients']):\n",
    "      coefficients_summary.append({\n",
    "          'Fold': i + 1,\n",
    "          'Factor': f'Factor_{j+1}',\n",
    "          'Coefficient': coef\n",
    "      })\n",
    "\n",
    "coefficients_df = pd.DataFrame(coefficients_summary)\n",
    "coefficients_pivot = coefficients_df.pivot(index='Factor', columns='Fold', values='Coefficient')\n",
    "coefficients_pivot['Mean'] = coefficients_pivot.mean(axis=1)\n",
    "coefficients_pivot['Std'] = coefficients_pivot.std(axis=1)\n",
    "coefficients_pivot['CV'] = coefficients_pivot['Std'] / np.abs(coefficients_pivot['Mean'])  # 变异系数\n",
    "\n",
    "\n",
    "cv_results_df.to_csv(os.path.join(save_dir, 'cross_validation_results.csv'), index=False)\n",
    "summary_df.to_csv(os.path.join(save_dir, 'cross_validation_summary.csv'), index=False)\n",
    "coefficients_pivot.to_csv(os.path.join(save_dir, 'cross_validation_coefficients.csv'))\n",
    "\n",
    "\n",
    "for i, result in enumerate(cv_detailed_results):\n",
    "  fold_predictions_df = pd.DataFrame({\n",
    "      'Actual_Return': result['actual'],\n",
    "      'Predicted_Return': result['predictions'],\n",
    "      'Residual': result['residuals'],\n",
    "      'Abs_Residual': np.abs(result['residuals'])\n",
    "  })\n",
    "  fold_predictions_df.to_csv(os.path.join(save_dir, f'fold_{i+1}_predictions.csv'), index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"=== 交叉验证结果汇总 ===\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"R² - 均值: {cv_results_df['R_Squared'].mean():.4f} ± {cv_results_df['R_Squared'].std():.4f}\")\n",
    "print(f\"调整R² - 均值: {cv_results_df['Adjusted_R_Squared'].mean():.4f} ± {cv_results_df['Adjusted_R_Squared'].std():.4f}\")\n",
    "print(f\"MSE - 均值: {cv_results_df['MSE'].mean():.6f} ± {cv_results_df['MSE'].std():.6f}\")\n",
    "print(f\"信息比率 - 均值: {cv_results_df['Information_Ratio'].mean():.4f} ± {cv_results_df['Information_Ratio'].std():.4f}\")\n",
    "\n",
    "\n",
    "r2_cv_coef = cv_results_df['R_Squared'].std() / cv_results_df['R_Squared'].mean()\n",
    "print(f\"\\n=== 稳健性评估 ===\")\n",
    "print(f\"R²变异系数: {r2_cv_coef:.4f}\")\n",
    "print(f\"R²稳健性评估: {'稳健' if r2_cv_coef < 0.1 else '中等' if r2_cv_coef < 0.2 else '不够稳健'}\")\n",
    "\n",
    "print(f\"\\n=== 性能趋势分析 ===\")\n",
    "for i in range(3):\n",
    "  print(f\"第{i+1}折 (训练集占{cv_results_df.iloc[i]['Train_Ratio']:.1%}): R² = {cv_results_df.iloc[i]['R_Squared']:.4f}\")\n",
    "\n",
    "print(f\"\\n=== 保存的CSV文件 ===\")\n",
    "print(f\"1. 交叉验证主要结果: cross_validation_results.csv\")\n",
    "print(f\"2. 统计汇总: cross_validation_summary.csv\") \n",
    "print(f\"3. 因子系数汇总: cross_validation_coefficients.csv\")\n",
    "print(f\"4. 各折预测详情: fold_1_predictions.csv, fold_2_predictions.csv, fold_3_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8f2bb2-1462-4853-8322-69caa6cf8103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (901767, 36)\n",
      "Generated 528 interaction features\n",
      "\n",
      "============================================================\n",
      "扩展功能1: 测试25-35个因子的双结构神经网络R平方\n",
      "============================================================\n",
      "\n",
      "训练因子数量为 25 的模型...\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function train_step_time at 0x7fe8eff2f130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 17 calls to <function train_step_time at 0x7fe8eff2f130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  Epoch 50/300, Loss: 0.643653\n",
      "  Epoch 100/300, Loss: 0.553692\n",
      "  Epoch 150/300, Loss: 0.520674\n",
      "  Epoch 200/300, Loss: 0.489024\n",
      "  Epoch 250/300, Loss: 0.473093\n",
      "  Epoch 300/300, Loss: 0.465716\n",
      "  因子数量 25: R² = 0.2026, Adjusted R² = 0.2026\n",
      "\n",
      "训练因子数量为 26 的模型...\n",
      "  Epoch 50/300, Loss: 0.641906\n",
      "  Epoch 100/300, Loss: 0.553808\n",
      "  Epoch 150/300, Loss: 0.513077\n",
      "  Epoch 200/300, Loss: 0.491131\n",
      "  Epoch 250/300, Loss: 0.471027\n",
      "  Epoch 300/300, Loss: 0.463428\n",
      "  因子数量 26: R² = 0.2019, Adjusted R² = 0.2019\n",
      "\n",
      "训练因子数量为 27 的模型...\n",
      "  Epoch 50/300, Loss: 0.646159\n",
      "  Epoch 100/300, Loss: 0.564456\n",
      "  Epoch 150/300, Loss: 0.521822\n",
      "  Epoch 200/300, Loss: 0.509633\n",
      "  Epoch 250/300, Loss: 0.481696\n",
      "  Epoch 300/300, Loss: 0.465961\n",
      "  因子数量 27: R² = 0.2024, Adjusted R² = 0.2024\n",
      "\n",
      "训练因子数量为 28 的模型...\n",
      "  Epoch 50/300, Loss: 0.637951\n",
      "  Epoch 100/300, Loss: 0.554683\n",
      "  Epoch 150/300, Loss: 0.513736\n",
      "  Epoch 200/300, Loss: 0.491519\n",
      "  Epoch 250/300, Loss: 0.475881\n",
      "  Epoch 300/300, Loss: 0.463568\n",
      "  因子数量 28: R² = 0.2024, Adjusted R² = 0.2024\n",
      "\n",
      "训练因子数量为 29 的模型...\n",
      "  Epoch 50/300, Loss: 0.654394\n",
      "  Epoch 100/300, Loss: 0.556293\n",
      "  Epoch 150/300, Loss: 0.508612\n",
      "  Epoch 200/300, Loss: 0.483819\n",
      "  Epoch 250/300, Loss: 0.472254\n",
      "  Epoch 300/300, Loss: 0.457701\n",
      "  因子数量 29: R² = 0.2025, Adjusted R² = 0.2024\n",
      "\n",
      "训练因子数量为 30 的模型...\n",
      "  Epoch 50/300, Loss: 0.646498\n",
      "  Epoch 100/300, Loss: 0.549239\n",
      "  Epoch 150/300, Loss: 0.516035\n",
      "  Epoch 200/300, Loss: 0.487378\n",
      "  Epoch 250/300, Loss: 0.469919\n",
      "  Epoch 300/300, Loss: 0.458525\n",
      "  因子数量 30: R² = 0.2022, Adjusted R² = 0.2022\n",
      "\n",
      "训练因子数量为 31 的模型...\n",
      "  Epoch 50/300, Loss: 0.680608\n",
      "  Epoch 100/300, Loss: 0.563620\n",
      "  Epoch 150/300, Loss: 0.532876\n",
      "  Epoch 200/300, Loss: 0.503259\n",
      "  Epoch 250/300, Loss: 0.483640\n",
      "  Epoch 300/300, Loss: 0.470450\n",
      "  因子数量 31: R² = 0.1998, Adjusted R² = 0.1998\n",
      "\n",
      "训练因子数量为 32 的模型...\n",
      "  Epoch 50/300, Loss: 0.636717\n",
      "  Epoch 100/300, Loss: 0.553257\n",
      "  Epoch 150/300, Loss: 0.512324\n",
      "  Epoch 200/300, Loss: 0.491237\n",
      "  Epoch 250/300, Loss: 0.465050\n",
      "  Epoch 300/300, Loss: 0.462163\n",
      "  因子数量 32: R² = 0.2027, Adjusted R² = 0.2027\n",
      "\n",
      "训练因子数量为 33 的模型...\n",
      "  Epoch 50/300, Loss: 0.648475\n",
      "  Epoch 100/300, Loss: 0.561006\n",
      "  Epoch 150/300, Loss: 0.522952\n",
      "  Epoch 200/300, Loss: 0.496882\n",
      "  Epoch 250/300, Loss: 0.480654\n",
      "  Epoch 300/300, Loss: 0.467554\n",
      "  因子数量 33: R² = 0.2038, Adjusted R² = 0.2037\n",
      "\n",
      "训练因子数量为 34 的模型...\n",
      "  Epoch 50/300, Loss: 0.635430\n",
      "  Epoch 100/300, Loss: 0.548850\n",
      "  Epoch 150/300, Loss: 0.506890\n",
      "  Epoch 200/300, Loss: 0.483485\n",
      "  Epoch 250/300, Loss: 0.466801\n",
      "  Epoch 300/300, Loss: 0.454436\n",
      "  因子数量 34: R² = 0.2024, Adjusted R² = 0.2024\n",
      "\n",
      "训练因子数量为 35 的模型...\n",
      "  Epoch 50/300, Loss: 0.641510\n",
      "  Epoch 100/300, Loss: 0.551993\n",
      "  Epoch 150/300, Loss: 0.518316\n",
      "  Epoch 200/300, Loss: 0.494557\n",
      "  Epoch 250/300, Loss: 0.476920\n",
      "  Epoch 300/300, Loss: 0.458401\n",
      "  因子数量 35: R² = 0.2022, Adjusted R² = 0.2022\n",
      "\n",
      "============================================================\n",
      "25-35个因子的R²统计结果汇总\n",
      "============================================================\n",
      "\n",
      "详细结果表:\n",
      "因子数量               R²         调整R²        观测数\n",
      "--------------------------------------------------\n",
      "25.0         0.202628     0.202606     901767\n",
      "26.0         0.201947     0.201924     901767\n",
      "27.0         0.202424     0.202401     901767\n",
      "28.0         0.202393     0.202368     901767\n",
      "29.0         0.202454     0.202429     901767\n",
      "30.0         0.202199     0.202173     901767\n",
      "31.0         0.199780     0.199753     901767\n",
      "32.0         0.202730     0.202702     901767\n",
      "33.0         0.203752     0.203723     901767\n",
      "34.0         0.202381     0.202351     901767\n",
      "35.0         0.202206     0.202175     901767\n",
      "\n",
      "性能分析:\n",
      "最高R²: 33个因子 (R² = 0.203752)\n",
      "最高调整R²: 33个因子 (调整R² = 0.203723)\n",
      "\n",
      "R²统计:\n",
      "  平均值: 0.202263\n",
      "  标准差: 0.000901\n",
      "  最小值: 0.199780\n",
      "  最大值: 0.203752\n",
      "\n",
      "调整R²统计:\n",
      "  平均值: 0.202237\n",
      "  标准差: 0.000901\n",
      "  最小值: 0.199753\n",
      "  最大值: 0.203723\n",
      "\n",
      "============================================================\n",
      "实验完成\n",
      "============================================================\n",
      "结果已保存至目录: /Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/factors_25_35_results\n",
      "- CSV文件: factors_25_35_r_squared_results.csv\n",
      "- JSON文件: factors_25_35_complete_results.json\n",
      "\n",
      "共训练了 11 个模型，因子数量范围: 25-35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "data = pd.read_csv('/Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/DL_Data16_processed.csv')\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "\n",
    "\n",
    "numeric_cols = data.columns[2:]  # \n",
    "data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())\n",
    "\n",
    "\n",
    "original_feature_names = list(data.columns[3:36])  # \n",
    "\n",
    "\n",
    "original_features = data.columns[3:36]  \n",
    "interaction_features = []\n",
    "interaction_feature_names = []\n",
    "\n",
    "for i, j in combinations(range(len(original_features)), 2):\n",
    "    col_name = f'interaction_{i}_{j}'\n",
    "    interaction_name = f'{original_features[i]}_×_{original_features[j]}'\n",
    "    data[col_name] = data[original_features[i]] * data[original_features[j]]\n",
    "    interaction_features.append(col_name)\n",
    "    interaction_feature_names.append(interaction_name)\n",
    "\n",
    "print(f\"Generated {len(interaction_features)} interaction features\")\n",
    "\n",
    "\n",
    "all_feature_names = original_feature_names + interaction_feature_names\n",
    "\n",
    "all_features = list(original_features) + interaction_features\n",
    "X = data[all_features].values\n",
    "y = data['Return'].values\n",
    "stock_ids = data.iloc[:, 0].values\n",
    "time_ids = data.iloc[:, 1].values\n",
    "\n",
    "\n",
    "train_idx = int(len(data) * 0.8)\n",
    "X_train, X_test = X[:train_idx], X[train_idx:]\n",
    "y_train, y_test = y[:train_idx], y[train_idx:]\n",
    "time_train, time_test = time_ids[:train_idx], time_ids[train_idx:]\n",
    "stock_train, stock_test = stock_ids[:train_idx], stock_ids[train_idx:]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "class DualStructureNN(keras.Model):\n",
    "    def __init__(self, num_factors=10, num_features=None,\n",
    "                 factor_loading_layers=[256, 128], \n",
    "                 factor_extraction_layers=[256, 128]):\n",
    "        super(DualStructureNN, self).__init__()\n",
    "        self.num_factors = num_factors\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # 因子载荷网络 g(z_i,t; θ)\n",
    "        self.factor_loading_layers = []\n",
    "        for i, units in enumerate(factor_loading_layers):\n",
    "            self.factor_loading_layers.append(\n",
    "                layers.Dense(units, activation='relu', name=f'g_layer_{i}')\n",
    "            )\n",
    "        self.factor_loading_output = layers.Dense(\n",
    "            num_factors, activation=None, name='g_output'\n",
    "        )\n",
    "        \n",
    "        # 因子提取网络 h(x_t+1; φ)\n",
    "        self.factor_extraction_layers = []\n",
    "        for i, units in enumerate(factor_extraction_layers):\n",
    "            self.factor_extraction_layers.append(\n",
    "                layers.Dense(units, activation='relu', name=f'h_layer_{i}')\n",
    "            )\n",
    "        self.factor_extraction_output = layers.Dense(\n",
    "            num_factors, activation=None, name='h_output'\n",
    "        )\n",
    "        \n",
    "    def compute_factor_loadings(self, z):\n",
    "        \"\"\"计算因子载荷 β_i,t = g(z_i,t; θ)\"\"\"\n",
    "        x = z\n",
    "        for layer in self.factor_loading_layers:\n",
    "            x = layer(x)\n",
    "        return self.factor_loading_output(x)\n",
    "    \n",
    "    def compute_factors(self, x_weighted):\n",
    "        \"\"\"计算因子 f_t+1 = h(x_t+1; φ)\"\"\"\n",
    "        # 确保输入是二维的\n",
    "        if len(x_weighted.shape) == 1:\n",
    "            x_weighted = tf.expand_dims(x_weighted, 0)\n",
    "        \n",
    "        x = x_weighted\n",
    "        for layer in self.factor_extraction_layers:\n",
    "            x = layer(x)\n",
    "        factors = self.factor_extraction_output(x)\n",
    "        \n",
    "        # 如果输入是单个样本，返回一维向量\n",
    "        if factors.shape[0] == 1:\n",
    "            factors = tf.squeeze(factors, axis=0)\n",
    "            \n",
    "        return factors\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        z_batch, x_weighted = inputs\n",
    "        \n",
    "        # 计算因子载荷\n",
    "        factor_loadings = self.compute_factor_loadings(z_batch)\n",
    "        \n",
    "        # 计算因子\n",
    "        factors = self.compute_factors(x_weighted)\n",
    "        \n",
    "        # 计算预测收益率\n",
    "        # 如果factors是一维的，需要广播到batch size\n",
    "        if len(factors.shape) == 1:\n",
    "            factors = tf.expand_dims(factors, 0)\n",
    "            factors = tf.tile(factors, [tf.shape(factor_loadings)[0], 1])\n",
    "        \n",
    "        predictions = tf.reduce_sum(factor_loadings * factors, axis=1)\n",
    "        \n",
    "        return predictions, factor_loadings, factors\n",
    "\n",
    "\n",
    "def prepare_time_based_data(X, y, time_ids):\n",
    "    \"\"\"按时间组织数据\"\"\"\n",
    "    unique_times = np.unique(time_ids)\n",
    "    time_data = {}\n",
    "    \n",
    "    for t in unique_times:\n",
    "        mask = time_ids == t\n",
    "        time_data[t] = {\n",
    "            'X': X[mask],\n",
    "            'y': y[mask],\n",
    "            'indices': np.where(mask)[0]\n",
    "        }\n",
    "    \n",
    "    return time_data, unique_times\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"扩展功能1: 测试25-35个因子的双结构神经网络R平方\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "factor_numbers = list(range(25, 36))  # 25到35个因子\n",
    "nn_results = {}\n",
    "\n",
    "for num_factors in factor_numbers:\n",
    "    print(f\"\\n训练因子数量为 {num_factors} 的模型...\")\n",
    "    \n",
    "    model = DualStructureNN(\n",
    "        num_factors=num_factors, \n",
    "        num_features=X_train_scaled.shape[1],\n",
    "        factor_loading_layers=[256, 128, 64],\n",
    "        factor_extraction_layers=[256, 128, 64]\n",
    "    )\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    train_time_data, unique_times_train = prepare_time_based_data(\n",
    "        X_train_scaled, y_train, time_train\n",
    "    )\n",
    " \n",
    "    @tf.function\n",
    "    def train_step_time(z_batch, y_batch, x_weighted):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, _, _ = model([z_batch, x_weighted], training=True)\n",
    "            loss = tf.reduce_mean(tf.square(y_batch - predictions))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    " \n",
    "    epochs = 300\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for t in unique_times_train:\n",
    "            t_data = train_time_data[t]\n",
    "            if len(t_data['X']) < 2:\n",
    "                continue\n",
    "                \n",
    "            x_weighted = np.dot(t_data['X'].T, t_data['y']) / len(t_data['y'])\n",
    "            \n",
    "            loss = train_step_time(\n",
    "                tf.constant(t_data['X'], dtype=tf.float32),\n",
    "                tf.constant(t_data['y'], dtype=tf.float32),\n",
    "                tf.constant(x_weighted, dtype=tf.float32)\n",
    "            )\n",
    "            epoch_losses.append(loss.numpy())\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch + 1}/{epochs}, Loss: {np.mean(epoch_losses):.6f}\")\n",
    " \n",
    "    X_full_scaled = np.vstack([X_train_scaled, X_test_scaled])\n",
    "    y_full = np.concatenate([y_train, y_test])\n",
    "    time_full = np.concatenate([time_train, time_test])\n",
    "    \n",
    "    all_factors_full = []\n",
    "    full_factor_indices = []\n",
    "    \n",
    "    full_time_data, unique_times_full = prepare_time_based_data(\n",
    "        X_full_scaled, y_full, time_full\n",
    "    )\n",
    "    \n",
    "    for t in unique_times_full:\n",
    "        t_data = full_time_data[t]\n",
    "        if len(t_data['X']) < 2:\n",
    "            continue\n",
    "        \n",
    "        x_weighted = np.dot(t_data['X'].T, t_data['y']) / len(t_data['y'])\n",
    "        \n",
    "        _, _, factors = model(\n",
    "            [tf.constant(t_data['X'], dtype=tf.float32),\n",
    "             tf.constant(x_weighted, dtype=tf.float32)],\n",
    "            training=False\n",
    "        )\n",
    "        \n",
    "        factors_np = factors.numpy()\n",
    "        if len(factors_np.shape) == 1:\n",
    "            factors_expanded = np.tile(factors_np, (len(t_data['X']), 1))\n",
    "        else:\n",
    "            factors_expanded = factors_np\n",
    "        \n",
    "        all_factors_full.extend(factors_expanded)\n",
    "        full_factor_indices.extend(t_data['indices'])\n",
    "    \n",
    "    all_factors_full = np.array(all_factors_full)\n",
    "    \n",
    " \n",
    "    X_factors_full = all_factors_full\n",
    "    y_returns_full = y_full[full_factor_indices]\n",
    "    \n",
    "    linear_reg = LinearRegression()\n",
    "    linear_reg.fit(X_factors_full, y_returns_full)\n",
    "    y_pred_full = linear_reg.predict(X_factors_full)\n",
    "    r2_full = r2_score(y_returns_full, y_pred_full)\n",
    "    \n",
    "    n = len(y_returns_full)\n",
    "    p = num_factors\n",
    "    adj_r2_full = 1 - (1 - r2_full) * (n - 1) / (n - p - 1)\n",
    "    \n",
    "    nn_results[num_factors] = {\n",
    "        'r2': r2_full,\n",
    "        'adj_r2': adj_r2_full,\n",
    "        'n_observations': n,\n",
    "        'n_factors': p\n",
    "    }\n",
    "    \n",
    "    print(f\"  因子数量 {num_factors}: R² = {r2_full:.4f}, Adjusted R² = {adj_r2_full:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"25-35个因子的R²统计结果汇总\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "results_table = []\n",
    "for num_factors in factor_numbers:\n",
    "    result = nn_results[num_factors]\n",
    "    results_table.append({\n",
    "        'Factors': num_factors,\n",
    "        'R_squared': result['r2'],\n",
    "        'Adj_R_squared': result['adj_r2'],\n",
    "        'N_observations': result['n_observations']\n",
    "    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(\"\\n详细结果表:\")\n",
    "print(f\"{'因子数量':<8} {'R²':>12} {'调整R²':>12} {'观测数':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in results_df.iterrows():\n",
    "    print(f\"{row['Factors']:<8} {row['R_squared']:>12.6f} {row['Adj_R_squared']:>12.6f} {row['N_observations']:>10.0f}\")\n",
    "\n",
    "\n",
    "best_r2_idx = results_df['R_squared'].idxmax()\n",
    "best_adj_r2_idx = results_df['Adj_R_squared'].idxmax()\n",
    "\n",
    "print(f\"\\n性能分析:\")\n",
    "print(f\"最高R²: {results_df.loc[best_r2_idx, 'Factors']}个因子 (R² = {results_df.loc[best_r2_idx, 'R_squared']:.6f})\")\n",
    "print(f\"最高调整R²: {results_df.loc[best_adj_r2_idx, 'Factors']}个因子 (调整R² = {results_df.loc[best_adj_r2_idx, 'Adj_R_squared']:.6f})\")\n",
    "\n",
    "\n",
    "r2_values = results_df['R_squared'].values\n",
    "adj_r2_values = results_df['Adj_R_squared'].values\n",
    "\n",
    "print(f\"\\nR²统计:\")\n",
    "print(f\"  平均值: {np.mean(r2_values):.6f}\")\n",
    "print(f\"  标准差: {np.std(r2_values):.6f}\")\n",
    "print(f\"  最小值: {np.min(r2_values):.6f}\")\n",
    "print(f\"  最大值: {np.max(r2_values):.6f}\")\n",
    "\n",
    "print(f\"\\n调整R²统计:\")\n",
    "print(f\"  平均值: {np.mean(adj_r2_values):.6f}\")\n",
    "print(f\"  标准差: {np.std(adj_r2_values):.6f}\")\n",
    "print(f\"  最小值: {np.min(adj_r2_values):.6f}\")\n",
    "print(f\"  最大值: {np.max(adj_r2_values):.6f}\")\n",
    "\n",
    "\n",
    "save_dir = '/Users/xiaoquanliu/Desktop/Book_DataCode1/第七章/factors_25_35_results'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "results_df.to_csv(os.path.join(save_dir, 'factors_25_35_r_squared_results.csv'), index=False)\n",
    "\n",
    "\n",
    "complete_results = {\n",
    "    'experiment_info': {\n",
    "        'factor_range': [25, 35],\n",
    "        'total_models_trained': len(factor_numbers),\n",
    "        'training_epochs': 300,\n",
    "        'train_test_split': 0.8\n",
    "    },\n",
    "    'results_summary': {\n",
    "        'best_r2_factors': int(results_df.loc[best_r2_idx, 'Factors']),\n",
    "        'best_r2_value': float(results_df.loc[best_r2_idx, 'R_squared']),\n",
    "        'best_adj_r2_factors': int(results_df.loc[best_adj_r2_idx, 'Factors']),\n",
    "        'best_adj_r2_value': float(results_df.loc[best_adj_r2_idx, 'Adj_R_squared']),\n",
    "        'r2_statistics': {\n",
    "            'mean': float(np.mean(r2_values)),\n",
    "            'std': float(np.std(r2_values)),\n",
    "            'min': float(np.min(r2_values)),\n",
    "            'max': float(np.max(r2_values))\n",
    "        },\n",
    "        'adj_r2_statistics': {\n",
    "            'mean': float(np.mean(adj_r2_values)),\n",
    "            'std': float(np.std(adj_r2_values)),\n",
    "            'min': float(np.min(adj_r2_values)),\n",
    "            'max': float(np.max(adj_r2_values))\n",
    "        }\n",
    "    },\n",
    "    'detailed_results': {\n",
    "        str(k): {\n",
    "            'r2': float(v['r2']),\n",
    "            'adj_r2': float(v['adj_r2']),\n",
    "            'n_observations': int(v['n_observations']),\n",
    "            'n_factors': int(v['n_factors'])\n",
    "        } for k, v in nn_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, 'factors_25_35_complete_results.json'), 'w') as f:\n",
    "    json.dump(complete_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"实验完成\")\n",
    "print(\"=\"*60)\n",
    "print(f\"结果已保存至目录: {save_dir}\")\n",
    "print(f\"- CSV文件: factors_25_35_r_squared_results.csv\")\n",
    "print(f\"- JSON文件: factors_25_35_complete_results.json\")\n",
    "print(f\"\\n共训练了 {len(factor_numbers)} 个模型，因子数量范围: {min(factor_numbers)}-{max(factor_numbers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e607a-a44e-4619-af4c-dd945a292ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
